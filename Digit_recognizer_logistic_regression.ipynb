{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    sm = (np.exp(z).T / np.sum(np.exp(z), axis =1))\n",
    "    return(sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(dim1, dim2):\n",
    "    \"\"\"\n",
    "        :param dim: size of vector w initialized with zeros\n",
    "        \n",
    "    \"\"\"\n",
    "    w = np.zeros(shape =(dim1, dim2))\n",
    "    b = np.zeros(shape =(10, 1))\n",
    "    return(w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "        :param w: weights for w\n",
    "        :param b: bias\n",
    "        :param X: size of data(no. of features, no. of examples)\n",
    "        :param Y: true label\n",
    "    \"\"\"\n",
    "    m = X.shape[1] # getting number of row\n",
    "    \n",
    "    ## Forward propagation\n",
    "    A = softmax((np.dot(w.T, X) + b).T)\n",
    "    cost = (-1/m) * np.sum(Y * np.log(A))\n",
    "    \n",
    "    ## Backward Propagation\n",
    "    dw = (1/m) * np.dot(X, (A - Y).T)\n",
    "    db = (1/m) * np.sum(A - Y)\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    grads = {\"dw\": dw,\n",
    "              \"db\":db}\n",
    "    return(grads, cost)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iters, alpha, print_cost = False):\n",
    "    \"\"\"\n",
    "        :param w: weight for w\n",
    "        :param b: bias\n",
    "        :param X: size of data(number of features, number of examples)\n",
    "        :param Y: true label\n",
    "        :param num_iters: number of iterations for gradient\n",
    "        :param alpha:\n",
    "        :return:\n",
    "        \n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    for i in range(num_iters):\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        w = w - alpha * dw\n",
    "        b = b - alpha * db\n",
    "        \n",
    "        # Record the costs\n",
    "        if i%50 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i%50 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            \n",
    "            \n",
    "    params = {\"w\": w,\n",
    "             \"b\": b} \n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "            \"db\": db}\n",
    "    \n",
    "    return(params,grads,costs)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    \"\"\"\n",
    "        :param w:\n",
    "        :param b:\n",
    "        :param X:\n",
    "    \"\"\"\n",
    "    \n",
    "    y_pred = np.argmax(softmax((np.dot(w.T, X) + b) .T), axis = 0)\n",
    "    return(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, Y ,X_test, Y_test, num_iters, alpha, print_cost):\n",
    "    \n",
    "    \"\"\"\n",
    "        :param X_train:\n",
    "        :param Y_train:\n",
    "        :param X_test:\n",
    "        :param Y_test:\n",
    "        :param num_iterations:\n",
    "        :param learning_rate:\n",
    "        :param print_cost:\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Logistic Regression Result\")\n",
    "    w,b = initialize(X_train.shape[0], Y_train.shape[0])\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iters, alpha, print_cost)\n",
    "    \n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    y_prediction_train = predict(w, b, X_train)\n",
    "    y_prediction_test = predict(w, b, X_test)\n",
    "    print(\"Train_accuracy: {} %\", sum(y_prediction_train == Y) /(float(len(Y))) * 100)\n",
    "    print(\"Test accuracy: {} %\", sum(y_prediction_test == Y_test) / (float(len(Y_test))) *100)\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "       \"Y_prediction_test\": y_prediction_test,\n",
    "       \"Y_prediction_train\": y_prediction_train,\n",
    "       \"w\": w,\n",
    "       \"b\": b,\n",
    "       \"learning_rate\": alpha,\n",
    "       \"num_iterations\": num_iters}\n",
    "    \n",
    "    return(d)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pri(X_test, y_prediction_test):\n",
    "    example = X_test[2,:]\n",
    "    print(\"Prediction for the example is \", y_prediction_test[2])\n",
    "    plt.imshow(np.reshape(example, [28,28]))\n",
    "    plt.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
